{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "116c09c1",
   "metadata": {},
   "source": [
    "# Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad1baf9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "\n",
    "df = pd.read_csv('cosmetics.csv')  \n",
    "df = df.dropna(subset=['Ingredients']) \n",
    "df['Ingredients'] = df['Ingredients'].str.lower()\n",
    "df['Ingredients'] = df['Ingredients'].str.replace(r'\\baqua\\b', 'water', regex=True)\n",
    "syn_dict = {\n",
    "    'vitamin e': 'tocopherol',\n",
    "}\n",
    "def unify_synonyms(ingr_str):\n",
    "    for k, v in syn_dict.items():\n",
    "        ingr_str = re.sub(r'\\b{}\\b'.format(k), v, ingr_str)\n",
    "    return ingr_str\n",
    "\n",
    "df['Ingredients'] = df['Ingredients'].apply(unify_synonyms)\n",
    "print(df['Ingredients'].head(10))\n",
    "df['ingredient_list'] = df['Ingredients'].apply(lambda x: [i.strip() for i in x.split(',') if i.strip() != ''])\n",
    "mlb = MultiLabelBinarizer()\n",
    "ingredient_matrix = mlb.fit_transform(df['ingredient_list'])\n",
    "ingredient_df = pd.DataFrame(ingredient_matrix, columns=mlb.classes_, index=df.index)\n",
    "df_final = pd.concat([df, ingredient_df], axis=1)\n",
    "print(df_final.head())\n",
    "print(df_final.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7556a2f1",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b50b2802",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.set_style(\"whitegrid\", {'grid.linestyle': '--'})\n",
    "sns.set_palette(\"pastel\")\n",
    "plt.rcParams['font.family'] = 'DejaVu Sans' \n",
    "category_counts = df['Label'].value_counts()\n",
    "plt.figure(figsize=(8, 8))\n",
    "explode = [0.03] * len(category_counts) \n",
    "wedges, texts, autotexts = plt.pie(\n",
    "    category_counts,\n",
    "    labels=category_counts.index,\n",
    "    autopct='%1.2f%%',\n",
    "    startangle=140,\n",
    "    pctdistance=0.8,\n",
    "    explode=explode,\n",
    "    textprops={'fontsize': 10, 'color': 'black'},\n",
    "    wedgeprops={'linewidth': 1, 'edgecolor': 'white', 'alpha': 0.9},\n",
    "    colors=sns.color_palette(\"pastel\")  \n",
    ")\n",
    "for autotext in autotexts:\n",
    "    autotext.set_size(10)\n",
    "    autotext.set_color('darkblue')\n",
    "\n",
    "centre_circle = plt.Circle((0,0), 0.6, fc='white')\n",
    "plt.gca().add_artist(centre_circle)\n",
    "\n",
    "plt.legend(\n",
    "    wedges,\n",
    "    category_counts.index,\n",
    "    title=\"Categories\",\n",
    "    loc=\"center right\",\n",
    "    bbox_to_anchor=(1, 0, 0.5, 1),\n",
    "    frameon=False\n",
    ")\n",
    "plt.title('Skincare Products Category Distribution\\n', \n",
    "          fontsize=14, \n",
    "          fontweight='bold', \n",
    "          color='#2E2E2E')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fae27cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "sns.set_style(\"whitegrid\", {'grid.linestyle': '--'})\n",
    "plt.rcParams['font.family'] = 'DejaVu Sans'\n",
    "colors = sns.color_palette(\"pastel\") \n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "categories_1 = ['Eye cream', 'Face Mask', 'Treatment']\n",
    "for i, cat in enumerate(categories_1):\n",
    "    subset = df[df['Label'] == cat]\n",
    "    sns.kdeplot(\n",
    "        data=subset, x='Price',\n",
    "        label=cat,\n",
    "        color=colors[i],        \n",
    "        fill=True,\n",
    "        alpha=0.6,             \n",
    "        linewidth=1.5,         \n",
    "        bw_adjust=0.65,  \n",
    "        ax=ax1\n",
    "    )\n",
    "\n",
    "ax1.set_xlabel('Price', fontsize=16, labelpad=10)\n",
    "ax1.set_ylabel('Density', fontsize=16, labelpad=10)\n",
    "ax1.set_title('Price Distribution: Core Treatments\\n', \n",
    "             fontsize=18, fontweight='bold', color='#2E2E2E')\n",
    "categories_2 = ['Moisturizer', 'Cleanser', 'Sun protect']\n",
    "for i, cat in enumerate(categories_2, start=3): \n",
    "    subset = df[df['Label'] == cat]\n",
    "    sns.kdeplot(\n",
    "        data=subset, x='Price',\n",
    "        label=cat,\n",
    "        color=colors[i],      \n",
    "        fill=True,\n",
    "        alpha=0.6,\n",
    "        linewidth=1.5,\n",
    "        bw_adjust=0.65,\n",
    "        ax=ax2\n",
    "    )\n",
    "\n",
    "ax2.set_xlabel('Price', fontsize=16, labelpad=10)\n",
    "ax2.set_ylabel('Density', fontsize=16, labelpad=10)\n",
    "ax2.set_title('Price Distribution: Basic Care\\n', \n",
    "             fontsize=18, fontweight='bold', color='#2E2E2E')\n",
    "\n",
    "for ax in [ax1, ax2]:\n",
    "    ax.grid(True, linestyle='--', alpha=0.6)\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['left'].set_color('#808080')\n",
    "    ax.spines['bottom'].set_color('#808080')\n",
    "    ax.legend(\n",
    "        title='Category',\n",
    "        title_fontsize=14,\n",
    "        fontsize=14,\n",
    "        frameon=False,\n",
    "        labelspacing=0.8,\n",
    "        bbox_to_anchor=(0.7, 0.7),  \n",
    "        loc='upper right'\n",
    "    )\n",
    "plt.tight_layout(pad=3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f6ab703",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "ingredient_cols = [col for col in df_final.columns \n",
    "                   if col not in ['Label', 'Brand', 'Name', 'Price', 'Rank', 'Ingredients', 'Combination',\n",
    "       'Dry', 'Normal', 'Oily', 'Sensitive','ingredient_list']]\n",
    "global_sum = df_final[ingredient_cols].sum().sort_values(ascending=False)\n",
    "top_10_ing = global_sum.head(10).index \n",
    "category_list = df_final['Label'].unique()\n",
    "freq_df = pd.DataFrame(0, index=category_list, columns=top_10_ing)\n",
    "\n",
    "for cat in category_list:\n",
    "    subset = df_final[df_final['Label'] == cat]\n",
    "    freq = subset[top_10_ing].sum()\n",
    "    freq_df.loc[cat] = freq\n",
    "plt.figure(figsize=(10, 6))\n",
    "freq_df.plot(kind='bar', figsize=(10,6))  \n",
    "plt.title('Top 10 Most Frequently Used Ingredients by Category')\n",
    "plt.xlabel('Product Category')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend(title='Ingredient', bbox_to_anchor=(1.05, 1))\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff08f431",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import math\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "ingredient_docs =df['Ingredients']\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform(ingredient_docs)  # shape = (n_products, n_terms)\n",
    "print(\"\\nTF-IDF shape:\", tfidf_matrix.shape)\n",
    "tsne = TSNE(n_components=2, random_state=42, perplexity=30, n_iter=1000)\n",
    "tfidf_emb = tsne.fit_transform(tfidf_matrix.toarray())  \n",
    "print(\"t-SNE embedding shape:\", tfidf_emb.shape)\n",
    "if 'category' in df.columns:\n",
    "    categories = df['category'].unique().tolist()\n",
    "    color_map = {cat: idx for idx, cat in enumerate(categories)}\n",
    "    c = df['category'].map(color_map).values\n",
    "    plt.figure(figsize=(7,5), dpi=120)\n",
    "    sc = plt.scatter(tfidf_emb[:,0], tfidf_emb[:,1], c=c, cmap='tab10', alpha=0.7)\n",
    "    plt.title(\"t-SNE of Cosmetics by Ingredients (colored by category)\")\n",
    "    plt.xlabel(\"Dim 1\")\n",
    "    plt.ylabel(\"Dim 2\")\n",
    "    cb = plt.colorbar(sc, ticks=range(len(categories)))\n",
    "    cb.ax.set_yticklabels(categories)  \n",
    "    plt.show()\n",
    "else:\n",
    "    plt.figure(figsize=(7,5), dpi=120)\n",
    "    plt.scatter(tfidf_emb[:,0], tfidf_emb[:,1], alpha=0.7)\n",
    "    plt.title(\"t-SNE of Cosmetics by Ingredients\")\n",
    "    plt.xlabel(\"Dim 1\")\n",
    "    plt.ylabel(\"Dim 2\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d67607f",
   "metadata": {},
   "source": [
    "# Jaccard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e6bb7de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import jaccard_score\n",
    "import pandas as pd\n",
    "\n",
    "def Jaccard(target_name):\n",
    "    ms_df = df_final[df_final['Label'] == \"Moisturizer\"].set_index('Name')\n",
    "    bin_df = ms_df.iloc[:, 12:]\n",
    "    if target_name not in bin_df.index:\n",
    "        raise ValueError(f\"no '{target_name}' \")\n",
    "    target_vec = bin_df.loc[target_name].values\n",
    "    sims = bin_df.apply(lambda row: \n",
    "                        jaccard_score(target_vec, row.values),\n",
    "                        axis=1)\n",
    "    sims = sims.drop(index=target_name)\n",
    "    top10 = sims.nlargest(10)\n",
    "    brands = ms_df.loc[top10.index, 'Brand'].values\n",
    "    prices = ms_df.loc[top10.index, 'Price'].values\n",
    "    result = pd.DataFrame({\n",
    "        'Product': top10.index,\n",
    "        'Brand':   brands,\n",
    "        'Price':   prices,\n",
    "        'Jaccard': top10.values\n",
    "    })\n",
    "    return result.reset_index(drop=True)\n",
    "print(Jaccard(\"Crème de la Mer\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd1cc14",
   "metadata": {},
   "source": [
    "# TFIDF_Cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e0dd91a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pandas as pd\n",
    "\n",
    "def TFIDF_Cosine(target_name):\n",
    "    ms_df = df_final[df_final['Label'] == \"Moisturizer\"].set_index('Name')\n",
    "    texts = ms_df['Ingredients'].tolist()\n",
    "    names = ms_df.index.tolist()\n",
    "    vectorizer = TfidfVectorizer(token_pattern=r\"(?u)\\b\\w+\\b\")\n",
    "    tfidf_matrix = vectorizer.fit_transform(texts)\n",
    "    if target_name not in names:\n",
    "        raise ValueError(f\"didn't find '{target_name}' \")\n",
    "    idx = names.index(target_name)\n",
    "    target_vec = tfidf_matrix[idx]\n",
    "    sims = cosine_similarity(tfidf_matrix, target_vec).flatten()\n",
    "    sims_series = pd.Series(sims, index=names).drop(index=target_name)\n",
    "    top10 = sims_series.nlargest(10)\n",
    "    brands = ms_df.loc[top10.index, 'Brand'].values\n",
    "    prices = ms_df.loc[top10.index, 'Price'].values\n",
    "    result = pd.DataFrame({\n",
    "        'Product':      top10.index,\n",
    "        'Brand':        brands,\n",
    "        'Price':        prices,\n",
    "        'CosineTFIDF':  top10.values\n",
    "    })\n",
    "    return result.reset_index(drop=True)\n",
    "\n",
    "TFIDF_Cosine(\"Crème de la Mer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da4247a0",
   "metadata": {},
   "source": [
    "# Bert Model From Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4f17142",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Carol\\anaconda\\lib\\site-packages\\huggingface_hub\\file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pandas as pd\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "bert_model = AutoModel.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "def _encode_texts(texts):\n",
    "    encoded = tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        out = bert_model(**encoded)\n",
    "    # out.last_hidden_state: [batch, seq_len, dim]\n",
    "    mask = encoded.attention_mask.unsqueeze(-1)  # [batch, seq_len, 1]\n",
    "    summed = (out.last_hidden_state * mask).sum(dim=1)   # [batch, dim]\n",
    "    counts = mask.sum(dim=1)                            # [batch, 1]\n",
    "    embeddings = summed / counts                        # mean-pooling\n",
    "    return embeddings.cpu().numpy()                  \n",
    "\n",
    "def BERT_Similarity(target_name,df_final):\n",
    "    ms_df = df_final[df_final['Label']==\"Moisturizer\"].set_index('Name')\n",
    "    if target_name not in ms_df.index:\n",
    "        raise ValueError(f\"no '{target_name}'\")\n",
    "    texts = ms_df['Ingredients'].tolist()\n",
    "    names = ms_df.index.tolist()\n",
    "    embeddings = _encode_texts(texts)\n",
    "    idx = names.index(target_name)\n",
    "    target_emb = embeddings[idx].reshape(1, -1)\n",
    "    sims = cosine_similarity(embeddings, target_emb).flatten()\n",
    "    sims_series = pd.Series(sims, index=names).drop(index=target_name)\n",
    "    top10 = sims_series.nlargest(10)\n",
    "    brands = ms_df.loc[top10.index, 'Brand'].values\n",
    "    prices = ms_df.loc[top10.index, 'Price'].values\n",
    "    return pd.DataFrame({\n",
    "        'Product':     top10.index,\n",
    "        'Brand':       brands,\n",
    "        'Price':       prices,\n",
    "        'CosineBERT':  top10.values\n",
    "    }).reset_index(drop=True)\n",
    "print(BERT_Similarity(\"Crème de la Mer\"))\n",
    "BERT_Similarity(\"Crème de la Mer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8387c711",
   "metadata": {},
   "source": [
    "## Model Improvements - Previously Ingredients had been preprocessed in Excel and products with incomplete ingredients were removed. The Ingredients column in the new df_new retains more non-standardized parts that need to be understood using semantics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf91ec81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def BERT_Similarity_new(target_name,df_final):\n",
    "    ms_df = df_final[df_final['Label']==\"Moisturizer\"].set_index('Name')\n",
    "    if target_name not in ms_df.index:\n",
    "        raise ValueError(f\"no '{target_name}' \")\n",
    "    texts = ms_df['Ingredients'].tolist()\n",
    "    names = ms_df.index.tolist()\n",
    "    embeddings = _encode_texts(texts)\n",
    "    idx = names.index(target_name)\n",
    "    target_emb = embeddings[idx].reshape(1, -1)\n",
    "    sims = cosine_similarity(embeddings, target_emb).flatten()\n",
    "    sims_series = pd.Series(sims, index=names).drop(index=target_name)\n",
    "    top10 = sims_series.nlargest(10)\n",
    "    brands = ms_df.loc[top10.index, 'Brand'].values\n",
    "    prices = ms_df.loc[top10.index, 'Price'].values\n",
    "    return pd.DataFrame({\n",
    "        'Product':     top10.index,\n",
    "        'Brand':       brands,\n",
    "        'Price':       prices,\n",
    "        'CosineBERT':  top10.values\n",
    "    }).reset_index(drop=True)\n",
    "df_new=pd.read_csv(\"cosmetics_new.csv\")\n",
    "print(BERT_Similarity_new(\"Crème de la Mer\",df_new))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e662bf83",
   "metadata": {},
   "source": [
    "## Further add core component identification to weight the components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82db2470",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "def BERT_Similarity_final(target_name, df_new, top_n=10):\n",
    "    df_moist = df_new[df_new['Label'] == 'Moisturizer']\n",
    "    target_row = df_moist[df_moist['Name'] == target_name]\n",
    "    if target_row.empty:\n",
    "        print(\"Target product not found in Moisturizer category.\")\n",
    "        return None\n",
    "    target_ingredients = str(target_row.iloc[0]['Ingredients'])\n",
    "    text = target_ingredients\n",
    "    text = re.sub(r'(?<=[A-Za-z])-\\s*(?=[A-Za-z])', '', text)\n",
    "    core_set = set()\n",
    "    if \"Active Ingredients:\" in text:\n",
    "        start_idx = text.index(\"Active Ingredients:\") + len(\"Active Ingredients:\")\n",
    "        remaining = text[start_idx:]\n",
    "        if \"Ingredients:\" in remaining:\n",
    "            actives_str = remaining.split(\"Ingredients:\")[0]\n",
    "        else:\n",
    "            actives_str = remaining\n",
    "        for ingr in actives_str.split(','):\n",
    "            ingr = ingr.strip()\n",
    "            if ingr:\n",
    "                ingr = ingr.lstrip(' -–').rstrip('.; ')\n",
    "                core_set.add(ingr)\n",
    "    elif ':' in text:\n",
    "        part = text.split(':')[0]\n",
    "        for ingr in part.split(','):\n",
    "            ingr = ingr.strip()\n",
    "            if ingr:\n",
    "                ingr = ingr.lstrip(' -–').rstrip('.; ')\n",
    "                core_set.add(ingr)\n",
    "    all_ingredients = [ing.strip() for ing in text.split(',') if ing.strip()]\n",
    "    if len(all_ingredients) < 5:\n",
    "        core_set.update(all_ingredients)\n",
    "    for ingr in all_ingredients:\n",
    "        if '%' in ingr:\n",
    "            raw = ingr.strip().rstrip('.,; ')\n",
    "            if '%' in raw:\n",
    "                name_part = raw.split('%')[0]\n",
    "                name_part = re.split(r'\\d', name_part)[0].strip()\n",
    "                ingr_name = name_part if name_part else raw\n",
    "            else:\n",
    "                ingr_name = raw\n",
    "            ingr_name = ingr_name.lstrip(' -–').rstrip('.; ')\n",
    "            if ingr_name:\n",
    "                core_set.add(ingr_name)\n",
    "    if not core_set:\n",
    "        core_set.update(all_ingredients)\n",
    "    core_text = ', '.join(core_set) if core_set else ''\n",
    "    tokenizer = AutoTokenizer.from_pretrained('bert-base-cased')\n",
    "    model = AutoModel.from_pretrained('bert-base-cased')\n",
    "    model.eval()\n",
    "    if core_text:\n",
    "        inputs = tokenizer(core_text, return_tensors='pt', truncation=True, padding=True)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            target_embedding = outputs.last_hidden_state[:, 0, :].squeeze(0)\n",
    "    else:\n",
    "        target_embedding = torch.zeros(model.config.hidden_size)\n",
    "    results = []\n",
    "    for _, row in tqdm(df_moist[df_moist['Name'] != target_name].iterrows(),\n",
    "                       total=len(df_moist[df_moist['Name'] != target_name]),\n",
    "                       desc=\"Computing similarities\"):\n",
    "        name = row['Name']; brand = row['Brand']; price = row['Price']\n",
    "        ing_text = str(row['Ingredients'])\n",
    "        t = re.sub(r'(?<=[A-Za-z])-\\s*(?=[A-Za-z])', '', ing_text)\n",
    "        core_set2 = set()\n",
    "        if \"Active Ingredients:\" in t:\n",
    "            si = t.index(\"Active Ingredients:\") + len(\"Active Ingredients:\")\n",
    "            rem = t[si:]\n",
    "            if \"Ingredients:\" in rem:\n",
    "                act_str = rem.split(\"Ingredients:\")[0]\n",
    "            else:\n",
    "                act_str = rem\n",
    "            for ingr in act_str.split(','):\n",
    "                ingr = ingr.strip()\n",
    "                if ingr:\n",
    "                    ingr = ingr.lstrip(' -–').rstrip('.; ')\n",
    "                    core_set2.add(ingr)\n",
    "        elif ':' in t:\n",
    "            part2 = t.split(':')[0]\n",
    "            for ingr in part2.split(','):\n",
    "                ingr = ingr.strip()\n",
    "                if ingr:\n",
    "                    ingr = ingr.lstrip(' -–').rstrip('.; ')\n",
    "                    core_set2.add(ingr)\n",
    "        all_ing2 = [ing.strip() for ing in t.split(',') if ing.strip()]\n",
    "        if len(all_ing2) < 5:\n",
    "            core_set2.update(all_ing2)\n",
    "        for ingr in all_ing2:\n",
    "            if '%' in ingr:\n",
    "                raw = ingr.strip().rstrip('.,; ')\n",
    "                if '%' in raw:\n",
    "                    name_part = raw.split('%')[0]\n",
    "                    name_part = re.split(r'\\d', name_part)[0].strip()\n",
    "                    ingr_name = name_part if name_part else raw\n",
    "                else:\n",
    "                    ingr_name = raw\n",
    "                ingr_name = ingr_name.lstrip(' -–').rstrip('.; ')\n",
    "                if ingr_name:\n",
    "                    core_set2.add(ingr_name)\n",
    "        if not core_set2:\n",
    "            core_set2.update(all_ing2)\n",
    "        core_text2 = ', '.join(core_set2) if core_set2 else ''\n",
    "        if core_text2:\n",
    "            inputs2 = tokenizer(core_text2, return_tensors='pt', truncation=True, padding=True)\n",
    "            with torch.no_grad():\n",
    "                outputs2 = model(**inputs2)\n",
    "                embed2 = outputs2.last_hidden_state[:, 0, :].squeeze(0)\n",
    "            if torch.norm(target_embedding) > 0 and torch.norm(embed2) > 0:\n",
    "                cos_sim = torch.dot(target_embedding, embed2) / (torch.norm(target_embedding) * torch.norm(embed2))\n",
    "                cos_sim = cos_sim.item()\n",
    "            else:\n",
    "                cos_sim = 0.0\n",
    "        else:\n",
    "            cos_sim = 0.0\n",
    "        results.append((name, brand, price, cos_sim))\n",
    "    results.sort(key=lambda x: x[3], reverse=True)\n",
    "    top_results = results[:top_n]\n",
    "    return pd.DataFrame(top_results, columns=['Name', 'Brand', 'Price', 'CosineSimilarity'])\n",
    "\n",
    "BERT_Similarity_final(\"Crème de la Mer\", df_new, top_n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4516e76e",
   "metadata": {},
   "source": [
    "## Robust test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdde6e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.stats import kendalltau\n",
    "from math import log2\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import jaccard_score\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def jaccard_recommend(df, target_name, top_n):\n",
    "    df_sub = df[df['Label']=='Moisturizer'].copy().set_index('Name')\n",
    "    ing_list = df_sub['Ingredients'].apply(lambda x: [i.strip() for i in x.split(',') if i.strip()])\n",
    "    mlb = MultiLabelBinarizer()\n",
    "    mat = mlb.fit_transform(ing_list)\n",
    "    bin_df = pd.DataFrame(mat, index=ing_list.index, columns=mlb.classes_)\n",
    "    target_vec = bin_df.loc[target_name].values\n",
    "    sims = bin_df.apply(lambda row: jaccard_score(target_vec, row.values), axis=1)\n",
    "    sims = sims.drop(index=target_name, errors='ignore')\n",
    "    return sims.nlargest(top_n).index.tolist()\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def tfidf_recommend(df, target_name, top_n):\n",
    "    df_sub = df[df['Label']=='Moisturizer'].copy()\n",
    "    names = df_sub['Name'].tolist()\n",
    "    texts = df_sub['Ingredients'].astype(str).tolist()\n",
    "    vect = TfidfVectorizer(token_pattern=r\"(?u)\\b\\w+\\b\", lowercase=True)\n",
    "    tfidf_mat = vect.fit_transform(texts)\n",
    "    idx = names.index(target_name)\n",
    "    sims = cosine_similarity(tfidf_mat, tfidf_mat[idx]).flatten()\n",
    "    sims_series = pd.Series(sims, index=names).drop(index=target_name, errors='ignore')\n",
    "    return sims_series.nlargest(top_n).index.tolist()\n",
    "\n",
    "def remove_noncore(ingredient_str, non_core_set):\n",
    "    ing_list = [ing.strip() for ing in ingredient_str.split(',') if ing.strip()]\n",
    "    core_list = [ing for ing in ing_list if ing.lower() not in non_core_set]\n",
    "    return \", \".join(core_list) if core_list else \", \".join(ing_list)\n",
    "\n",
    "def test_recommendation_robustness(df_new, top_n=10, non_core_set=None,\n",
    "                                   show_example=True, plot=True, random_state=42):\n",
    "    df_moist = df_new[df_new['Label']=='Moisturizer'].reset_index(drop=True)\n",
    "    train_df, test_df = train_test_split(df_moist, test_size=0.1, random_state=random_state)\n",
    "    if non_core_set is None:\n",
    "        all_ing = []\n",
    "        for txt in train_df['Ingredients']:\n",
    "            all_ing.extend([ing.strip().lower() for ing in txt.split(',') if ing.strip()])\n",
    "        non_core_set = set(pd.Series(all_ing).value_counts().head(10).index.tolist())\n",
    "    \n",
    "    metrics = []\n",
    "    example_done = False\n",
    "    for _, row in tqdm(test_df.iterrows(), total=len(test_df), desc=\"Robustness Eval\"):\n",
    "        name = row['Name']\n",
    "        orig_txt = row['Ingredients']\n",
    "        clean_txt = remove_noncore(orig_txt, non_core_set)\n",
    "        if show_example and not example_done:\n",
    "            #print(f\"\\n示例产品: {name}\")\n",
    "            #print(\"  原始成分:\", orig_txt)\n",
    "            #print(\"  清理后成分:\", clean_txt)\n",
    "            example_done = True\n",
    "        cand_orig = pd.concat([train_df, row.to_frame().T], ignore_index=True)\n",
    "        cand_clean = pd.concat([train_df, row.to_frame().T.assign(Ingredients=clean_txt)], ignore_index=True)\n",
    "        orig_j = jaccard_recommend(cand_orig, name, top_n)\n",
    "        clean_j = jaccard_recommend(cand_clean, name, top_n)\n",
    "        orig_t = tfidf_recommend(cand_orig, name, top_n)\n",
    "        clean_t = tfidf_recommend(cand_clean, name, top_n)\n",
    "        orig_b = list(BERT_Similarity_final(name, cand_orig, top_n=top_n)['Name'])\n",
    "        clean_b = list(BERT_Similarity_final(name, cand_clean, top_n=top_n)['Name'])\n",
    "        for method, L1, L2 in [\n",
    "            ('TFIDF', orig_j, clean_j),\n",
    "            ('BERT',  orig_t, clean_t),\n",
    "            ('Jaccard',  orig_b, clean_b)\n",
    "        ]:\n",
    "            # Overlap\n",
    "            overlap = len(set(L1)&set(L2))/top_n\n",
    "            # Rank Displacement\n",
    "            common = [i for i in L1 if i in L2]\n",
    "            disp = np.mean([abs(L1.index(i)-L2.index(i)) for i in common]) if common else top_n\n",
    "            # Kendall's tau\n",
    "            union = list(dict.fromkeys(L1+L2))\n",
    "            r1 = [L1.index(x)+1 if x in L1 else top_n+1 for x in union]\n",
    "            r2 = [L2.index(x)+1 if x in L2 else top_n+1 for x in union]\n",
    "            tau, _ = kendalltau(r1, r2)\n",
    "            # NDCG\n",
    "            DCG = sum((1 if item in L1 else 0)/log2(idx+2) for idx, item in enumerate(L2))\n",
    "            IDCG = sum(1/log2(i+2) for i in range(top_n))\n",
    "            ndcg = DCG/IDCG\n",
    "            \n",
    "            metrics.append({\n",
    "                'Method': method,\n",
    "                'Product': name,\n",
    "                'Overlap': overlap,\n",
    "                'RankDisp': disp,\n",
    "                'KendallTau': tau,\n",
    "                'NDCG': ndcg\n",
    "            })\n",
    "    \n",
    "    metrics_df = pd.DataFrame(metrics)\n",
    "    summary = metrics_df.groupby('Method').mean()[['Overlap','RankDisp','KendallTau','NDCG']]\n",
    "    display(summary)\n",
    "    if plot:\n",
    "        plt.figure(figsize=(6,4))\n",
    "        sns.barplot(data=summary.reset_index(), x='Method', y='Overlap')\n",
    "        plt.ylim(0,1); plt.title(f'Average Overlap (Top-{top_n})'); plt.show()\n",
    "        tau_mat = metrics_df.pivot(index='Product', columns='Method', values='KendallTau')\n",
    "        plt.figure(figsize=(8,6))\n",
    "        sns.heatmap(tau_mat, cmap='YlGnBu', annot=True, fmt=\".2f\")\n",
    "        plt.title(f'Kendall Tau per Sample (Top-{top_n})'); plt.show()\n",
    "    \n",
    "    return metrics_df, summary\n",
    "\n",
    "metrics_df, summary = test_recommendation_robustness(df_new, top_n=5, show_example=True, plot=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
